


@article{samuel1967some,
  title={Some studies in machine learning using the game of checkers. II—Recent progress},
  author={Samuel, Arthur L},
  journal={IBM Journal of research and development},
  volume={11},
  number={6},
  pages={601--617},
  year={1967},
  publisher={IBM},
  file={Docs/a04e17d3b9ea092bd7dd5295b2d61d53bff5.pdf}
}



@article{wu2019artificial,
	Author = {Wu, Shipher and Chang, Chun-Min and Mai, Guan-Shuo and Rubenstein, Dustin R. and Yang, Chen-Ming and Huang, Yu-Ting and Lin, Hsu-Hong and Shih, Li-Cheng and Chen, Sheng-Wei and Shen, Sheng-Feng},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {4554},
	Title = {Artificial intelligence reveals environmental constraints on colour diversity in insects},
	Volume = {10},
	Year = {2019},
	file={Docs/s41467-019-12500-2.pdf}
}





@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group},
  file={Docs/DeepLearning_LeCun.pdf}
}



@article{agostinelli2019solving,
  title={Solving the Rubik’s cube with deep reinforcement learning and search},
  author={Agostinelli, Forest and McAleer, Stephen and Shmakov, Alexander and Baldi, Pierre},
  journal={Nature Machine Intelligence},
  pages={1},
  year={2019},
  publisher={Nature Publishing Group},
  file={Docs/10.1038@s42256-019-0070-z.pdf}
}





@article{rocher2019estimating,
	Author = {Rocher, Luc and Hendrickx, Julien M. and de Montjoye, Yves-Alexandre},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {3069},
	Title = {Estimating the success of re-identifications in incomplete datasets using generative models},
	Volume = {10},
	Year = {2019},
	file={Docs/s41467-019-10933-3.pdf}
}





@article{fennell2019predicting,
  title={Predicting and explaining behavioral data with structured feature space decomposition},
  author={Fennell, Peter G and Zuo, Zhiya and Lerman, Kristina},
  journal={EPJ Data Science},
  volume={8},
  number={1},
  pages={23},
  year={2019},
  publisher={Springer},
  file={Docs/s13688-019-0201-0.pdf}
}




@article{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019},
  file={Docs/1906.02243.pdf}
}



@article{hirose2011efficient,
  title={Efficient algorithm to select tuning parameters in sparse regression modeling with regularization},
  author={Hirose, Kei and Tateishi, Shohei and Konishi, Sadanori},
  journal={arXiv preprint arXiv:1109.2411},
  year={2011},
  file={Docs/1109.2411.pdf}
}




@article{schmidt2009distilling,
  title={Distilling free-form natural laws from experimental data},
  author={Schmidt, Michael and Lipson, Hod},
  journal={science},
  volume={324},
  number={5923},
  pages={81--85},
  year={2009},
  publisher={American Association for the Advancement of Science},
  file={Docs/81.full.pdf}
}



@article{Fonda8667,
	author = {Fonda, Enrico and Pandey, Ambrish and Schumacher, J{\"o}rg and Sreenivasan, Katepalli R.},
	title = {Deep learning in turbulent convection networks},
	volume = {116},
	number = {18},
	pages = {8667--8672},
	year = {2019},
	doi = {10.1073/pnas.1900358116},
	publisher = {National Academy of Sciences},
	abstract = {Turbulent convection in horizontally extended systems comprises vortices and plumes on many time and length scales. These structures interact nonlinearly to self-organize into slowly evolving turbulent superstructures, which are horizontally more extended than in height. We use a U-shaped deep-learning algorithm to generate a time-varying planar network, resulting in a drastic reduction of degrees of freedom, and use it to detect the 3D superstructures and estimate their effectiveness in transporting heat. We thus demonstrate the likely utility of deep learning for parameterizing convection in global models of atmospheric and stellar convection whenever mesoscale structures are conspicuous.We explore heat transport properties of turbulent Rayleigh{\textendash}B{\'e}nard convection in horizontally extended systems by using deep-learning algorithms that greatly reduce the number of degrees of freedom. Particular attention is paid to the slowly evolving turbulent superstructures{\textemdash}so called because they are larger in extent than the height of the convection layer{\textemdash}which appear as temporal patterns of ridges of hot upwelling and cold downwelling fluid, including defects where the ridges merge or end. The machine-learning algorithm trains a deep convolutional neural network (CNN) with U-shaped architecture, consisting of a contraction and a subsequent expansion branch, to reduce the complex 3D turbulent superstructure to a temporal planar network in the midplane of the layer. This results in a data compression by more than five orders of magnitude at the highest Rayleigh number, and its application yields a discrete transport network with dynamically varying defect points, including points of locally enhanced heat flux or {\textquotedblleft}hot spots.{\textquotedblright} One conclusion is that the fraction of heat transport by the superstructure decreases as the Rayleigh number increases (although they might remain individually strong), correspondingly implying the increased importance of small-scale background turbulence.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/116/18/8667},
	eprint = {https://www.pnas.org/content/116/18/8667.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Docs/10.1073@pnas.1900358116.pdf}
}




@article{Krotov7723,
	author = {Krotov, Dmitry and Hopfield, John J.},
	title = {Unsupervised learning by competing hidden units},
	volume = {116},
	number = {16},
	pages = {7723--7731},
	year = {2019},
	doi = {10.1073/pnas.1820458116},
	publisher = {National Academy of Sciences},
	abstract = {Despite great success of deep learning a question remains to what extent the computational properties of deep neural networks are similar to those of the human brain. The particularly nonbiological aspect of deep learning is the supervised training process with the backpropagation algorithm, which requires massive amounts of labeled data, and a nonlocal learning rule for changing the synapse strengths. This paper describes a learning algorithm that does not suffer from these two problems. It learns the weights of the lower layer of neural networks in a completely unsupervised fashion. The entire algorithm utilizes local learning rules which have conceptual biological plausibility.It is widely believed that end-to-end training with the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility and which is motivated by Hebb{\textquoteright}s idea that change of the synapse strength should be local{\textemdash}i.e., should depend only on the activities of the pre- and postsynaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer and is capable of learning early feature detectors in a completely unsupervised way. These learned lower-layer feature detectors can be used to train higher-layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm on simple tasks.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/116/16/7723},
	eprint = {https://www.pnas.org/content/116/16/7723.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Docs/7723.full.pdf}
}



@ARTICLE{2019arXiv190404755F,
       author = {{Foster}, Dylan J. and {Greenberg}, Spencer and {Kale}, Satyen and
         {Luo}, Haipeng and {Mohri}, Mehryar and {Sridharan}, Karthik},
        title = "{Hypothesis Set Stability and Generalization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2019",
        month = "Apr",
          eid = {arXiv:1904.04755},
        pages = {arXiv:1904.04755},
archivePrefix = {arXiv},
       eprint = {1904.04755},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190404755F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	file={Docs/1904.04755.pdf}
}





@ARTICLE{2017arXiv171109846J,
       author = {{Jaderberg}, Max and {Dalibard}, Valentin and {Osindero}, Simon and
         {Czarnecki}, Wojciech M. and {Donahue}, Jeff and {Razavi}, Ali and
         {Vinyals}, Oriol and {Green}, Tim and {Dunning}, Iain and
         {Simonyan}, Karen and {Fernando}, Chrisantha and {Kavukcuoglu}, Koray},
        title = "{Population Based Training of Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2017",
        month = "Nov",
          eid = {arXiv:1711.09846},
        pages = {arXiv:1711.09846},
archivePrefix = {arXiv},
       eprint = {1711.09846},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109846J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	file={Docs/1711.09846.pdf}
}





@article{Barbier5451,
	author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
	title = {Optimal errors and phase transitions in high-dimensional generalized linear models},
	volume = {116},
	number = {12},
	pages = {5451--5460},
	year = {2019},
	doi = {10.1073/pnas.1802705116},
	publisher = {National Academy of Sciences},
	abstract = {High-dimensional generalized linear models are basic building blocks of current data analysis tools including multilayers neural networks. They arise in signal processing, statistical inference, machine learning, communication theory, and other fields. We establish rigorously the intrinsic information-theoretic limitations of inference and learning for a class of randomly generated instances of generalized linear models, thus closing several decades-old conjectures. Moreover, we delimit regions of parameters for which the optimal error rates are efficiently achievable with currently known algorithms. Our proof technique is able to deal with the output nonlinearity and is hence of independent interest, opening ways to establish similar results for models of neural networks where nonlinearities are essential but in general difficult to account for.Generalized linear models (GLMs) are used in high-dimensional machine learning, statistics, communications, and signal processing. In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes, or benchmark models in neural networks. We evaluate the mutual information (or {\textquotedblleft}free entropy{\textquotedblright}) from which we deduce the Bayes-optimal estimation and generalization errors. Our analysis applies to the high-dimensional limit where both the number of samples and the dimension are large and their ratio is fixed. Nonrigorous predictions for the optimal errors existed for special cases of GLMs, e.g., for the perceptron, in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades-old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance and locate the associated sharp phase transitions separating learnable and nonlearnable regions. We believe that this random version of GLMs can serve as a challenging benchmark for multipurpose algorithms.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/116/12/5451},
	eprint = {https://www.pnas.org/content/116/12/5451.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Docs/5451.full.pdf}
}


@article{Chen96,
	author = {Chen, Lichao and Singh, Sudhir and Kailath, Thomas and Roychowdhury, Vwani},
	title = {Brain-inspired automated visual object discovery and detection},
	volume = {116},
	number = {1},
	pages = {96--105},
	year = {2019},
	doi = {10.1073/pnas.1802103115},
	publisher = {National Academy of Sciences},
	abstract = {The Internet contains millions of videos and images of objects, captured contextually, just as they would appear in their natural environments. Such access to large-scale data has injected an urgency to an age-old query: Can one automatically browse these data, as a human would, to construct brain-like and computationally tractable models of the visual world? If successful, many of the scalability and robustness limitations of existing computer vision methodologies, which primarily rely on supervised frameworks requiring manually labeled exemplars, will be removed. As a step toward solving this problem, we leverage widely accepted findings in the fields of cognitive psychology and neurophysiology to develop an unsupervised framework for automated isolation of objects and their efficient detection and localization in new environments.Despite significant recent progress, machine vision systems lag considerably behind their biological counterparts in performance, scalability, and robustness. A distinctive hallmark of the brain is its ability to automatically discover and model objects, at multiscale resolutions, from repeated exposures to unlabeled contextual data and then to be able to robustly detect the learned objects under various nonideal circumstances, such as partial occlusion and different view angles. Replication of such capabilities in a machine would require three key ingredients: (i) access to large-scale perceptual data of the kind that humans experience, (ii) flexible representations of objects, and (iii) an efficient unsupervised learning algorithm. The Internet fortunately provides unprecedented access to vast amounts of visual data. This paper leverages the availability of such data to develop a scalable framework for unsupervised learning of object prototypes{\textemdash}brain-inspired flexible, scale, and shift invariant representations of deformable objects (e.g., humans, motorcycles, cars, airplanes) comprised of parts, their different configurations and views, and their spatial relationships. Computationally, the object prototypes are represented as geometric associative networks using probabilistic constructs such as Markov random fields. We apply our framework to various datasets and show that our approach is computationally scalable and can construct accurate and operational part-aware object models much more efficiently than in much of the recent computer vision literature. We also present efficient algorithms for detection and localization in new scenes of objects and their partial views.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/116/1/96},
	eprint = {https://www.pnas.org/content/116/1/96.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Docs/chen2018.pdf}
}



@ARTICLE{2018arXiv180203426M,
       author = {{McInnes}, Leland and {Healy}, John and {Melville}, James},
        title = "{UMAP: Uniform Manifold Approximation and Projection for Dimension
        Reduction}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Computational
        Geometry, Computer Science - Machine Learning},
         year = 2018,
        month = Feb,
          eid = {arXiv:1802.03426},
        pages = {arXiv:1802.03426},
archivePrefix = {arXiv},
       eprint = {1802.03426},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv180203426M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	file={Docs/1802.03426.pdf}
}





@article{MasseE10467,
	author = {Masse, Nicolas Y. and Grant, Gregory D. and Freedman, David J.},
	title = {Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization},
	volume = {115},
	number = {44},
	pages = {E10467--E10475},
	year = {2018},
	doi = {10.1073/pnas.1803839115},
	publisher = {National Academy of Sciences},
	abstract = {Artificial neural networks can suffer from catastrophic forgetting, in which learning a new task causes the network to forget how to perform previous tasks. While previous studies have proposed various methods that can alleviate forgetting over small numbers (⩽10) of tasks, it is uncertain whether they can prevent forgetting across larger numbers of tasks. In this study, we propose a neuroscience-inspired scheme, called {\textquotedblleft}context-dependent gating,{\textquotedblright} in which mostly nonoverlapping sets of units are active for any one task. Importantly, context-dependent gating has a straightforward implementation, requires little extra computational overhead, and when combined with previous methods to stabilize connection weights, can allow networks to maintain high performance across large numbers of sequentially presented tasks.Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks (ANNs) on new tasks typically causes them to forget previously learned tasks. This phenomenon is the result of {\textquotedblleft}catastrophic forgetting,{\textquotedblright} in which training an ANN disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of ANNs that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows ANNs to maintain high performance across large numbers of sequentially presented tasks, particularly when combined with weight stabilization. We show that this method works for both feedforward and recurrent network architectures, trained using either supervised or reinforcement-based learning. This suggests that using multiple, complementary methods, akin to what is believed to occur in the brain, can be a highly effective strategy to support continual learning.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/115/44/E10467},
	eprint = {http://www.pnas.org/content/115/44/E10467.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Docs/10.1073@pnas.1803839115.pdf}
}



@article{specht1990probabilistic,
  title={Probabilistic neural networks},
  author={Specht, Donald F},
  journal={Neural networks},
  volume={3},
  number={1},
  pages={109--118},
  year={1990},
  publisher={Elsevier},
  file={Docs/specht1990.pdf}
}



@Article{Ortin2015,
  author        = {Ortín, S. and Soriano, M. C. and Pesquera, L. and Brunner, D. and San-Martín, D. and Fischer, I. and Mirasso, C. R. and Gutiérrez, J. M.},
  title         = {A Unified Framework for Reservoir Computing and Extreme Learning Machines based on a Single Time-delayed Neuron},
  journal       = {Scientific Reports},
  year          = {2015},
  volume        = {5},
  pages         = {14945},
  month         = oct,
  publisher     = {The Author(s)},
  url           = {http://dx.doi.org/10.1038/srep14945},
  file={Docs/srep14945.pdf}
}




@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
  pages={1322--1333},
  year={2015},
  organization={ACM},
  file={Docs/fjr2015ccs.pdf}
}




@ARTICLE{2018arXiv180704644V,
   author = {{Veale}, M. and {Binns}, R. and {Edwards}, L.},
    title = "{Algorithms that Remember: Model Inversion Attacks and Data Protection Law}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1807.04644},
 keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
     year = 2018,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180704644V},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
    file = {Docs/1807.04644.pdf}
}





@article{marquez2018dynamical,
  title={Dynamical complexity and computation in recurrent neural networks beyond their fixed point},
  author={Marquez, Bicky A and Larger, Laurent and Jacquot, Maxime and Chembo, Yanne K and Brunner, Daniel},
  journal={Scientific reports},
  volume={8},
  number={1},
  pages={3319},
  year={2018},
  publisher={Nature Publishing Group},
  file={Docs/s41598-018-21624-2.pdf}
}


@article{gebru2017using,
  title={Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States},
  author={Gebru, Timnit and Krause, Jonathan and Wang, Yilun and Chen, Duyun and Deng, Jia and Aiden, Erez Lieberman and Fei-Fei, Li},
  journal={Proceedings of the National Academy of Sciences},
  pages={201700035},
  year={2017},
  publisher={National Acad Sciences},
  file={Docs/PNAS-2017-Gebru-1700035114.pdf}
}


@article{mikkelsen2018personalizing,
  title={Personalizing deep learning models for automatic sleep staging},
  author={Mikkelsen, Kaare and de Vos, Maarten},
  journal={arXiv preprint arXiv:1801.02645},
  year={2018},
  file={Docs/1801.02645.pdf}
}


@article{raposo2017towards,
  title={Towards Deep Modeling of Music Semantics using EEG Regularizers},
  author={Raposo, Francisco and de Matos, David Martins and Ribeiro, Ricardo and Tang, Suhua and Yu, Yi},
  journal={arXiv preprint arXiv:1712.05197},
  year={2017},
  file={Docs/1712.05197.pdf}
}


@ARTICLE{2018arXiv180600179P,
   author = {{Philipp}, G. and {Carbonell}, J.~G.},
    title = "{The Nonlinearity Coefficient - Predicting Overfitting in Deep Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.00179},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180600179P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  file={Docs/1806.00179.pdf}
}



@article{hausknecht2015deep,
	Author = {Hausknecht, Matthew and Stone, Peter},
	Date-Added = {2016-03-14 18:55:31 +0000},
	Date-Modified = {2016-03-14 18:55:31 +0000},
	Journal = {arXiv preprint arXiv:1511.04143},
	Title = {Deep Reinforcement Learning in Parameterized Action Space},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QFURvY3MvMTUxMS4wNDE0M3YxLnBkZtIXCxgZV05TLmRhdGFPEQIMAAAAAAIMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADKQQYMSCsAAAQoOF0QMTUxMS4wNDE0M3YxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6gDHtJvzCQAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABERvY3MAEAAIAADKQOnsAAAAEQAIAADSb74UAAAAAQAgBCg4XQQoOFwDsb35AKmYegCQwikABQIVAAUCFAAAvuwAAgB1TWFjaW50b3NoIEhEOlVzZXJzOgBKdXN0ZToARG9jdW1lbnRzOgBDb21wbGV4U3lzdGVtczoAQmlibGlvOgBEb21haW5TcGVjaWZpYzoATWFjaGluZUxlYXJuaW5nOgBEb2NzOgAxNTExLjA0MTQzdjEucGRmAAAOACIAEAAxADUAMQAxAC4AMAA0ADEANAAzAHYAMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAYFVzZXJzL0p1c3RlL0RvY3VtZW50cy9Db21wbGV4U3lzdGVtcy9CaWJsaW8vRG9tYWluU3BlY2lmaWMvTWFjaGluZUxlYXJuaW5nL0RvY3MvMTUxMS4wNDE0M3YxLnBkZgATAAEvAAAVAAIADP//AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAKYAqwCzAsMCxQLKAtUC3gLsAvAC9wMAAwUDEgMVAycDKgMvAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAzE=}}

@inproceedings{vinyals2015show,
	Author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
	Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	Date-Added = {2016-03-14 18:21:08 +0000},
	Date-Modified = {2016-03-14 18:21:08 +0000},
	Pages = {3156--3164},
	Title = {Show and tell: A neural image caption generator},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QFERvY3MvMTQxMS40NTU1djEucGRm0hcLGBlXTlMuZGF0YU8RAggAAAAAAggAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMpBBgxIKwAABCg4XQ8xNDExLjQ1NTV2MS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEQ020M9vawAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAERG9jcwAQAAgAAMpA6ewAAAARAAgAANDPYVsAAAABACAEKDhdBCg4XAOxvfkAqZh6AJDCKQAFAhUABQIUAAC+7AACAHRNYWNpbnRvc2ggSEQ6VXNlcnM6AEp1c3RlOgBEb2N1bWVudHM6AENvbXBsZXhTeXN0ZW1zOgBCaWJsaW86AERvbWFpblNwZWNpZmljOgBNYWNoaW5lTGVhcm5pbmc6AERvY3M6ADE0MTEuNDU1NXYxLnBkZgAOACAADwAxADQAMQAxAC4ANAA1ADUANQB2ADEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAF9Vc2Vycy9KdXN0ZS9Eb2N1bWVudHMvQ29tcGxleFN5c3RlbXMvQmlibGlvL0RvbWFpblNwZWNpZmljL01hY2hpbmVMZWFybmluZy9Eb2NzLzE0MTEuNDU1NXYxLnBkZgAAEwABLwAAFQACAAz//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgClAKoAsgK+AsACxQLQAtkC5wLrAvIC+wMAAw0DEAMiAyUDKgAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAMs}}
