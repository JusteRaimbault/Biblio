


@article{buck2021beware,
title={Beware performative reproducibility},
author={Buck, S.},
journal={Nature},
volume={595},
pages={151},
year={2021},
doi={10.1038/d41586-021-01824-z},
file={Biblio/DomainSpecific/Reproducibility/d41586-021-01824-z.pdf},
url={https://www.nature.com/articles/d41586-021-01824-z}
}




@article{amaral2021reproducibility,
author={Amaral, O.B. and Neves, K.},
title={Reproducibility: expect less of the scientific paper},
journal={Nature},
volume={597},
pages={329-331},
year={2021},
doi={10.1038/d41586-021-02486-7},
url={https://www.nature.com/articles/d41586-021-02486-7},
file={Biblio/DomainSpecific/Reproducibility/d41586-021-02486-7.pdf}
}



@article{heil2021reproducibility,
  title={Reproducibility standards for machine learning in the life sciences},
  author={Heil, Benjamin J and Hoffman, Michael M and Markowetz, Florian and Lee, Su-In and Greene, Casey S and Hicks, Stephanie C},
  journal={Nature Methods},
  pages={1--4},
  year={2021},
  publisher={Nature Publishing Group},
  file={Biblio/DomainSpecific/Reproducibility/s41592-021-01256-7.pdf}
}


@article{rowe2020potential,
  title={The Potential of Notebooks for Scientific Publication, Reproducibility and Dissemination},
  author={Rowe, Francisco and Maier, Gunther and Arribas-Bel, Daniel and Rey, Sergio},
  journal={REGION},
  volume={7},
  number={3},
  pages={E1--E5},
  year={2020},
  file={Biblio/DomainSpecific/Reproducibility/357-Article%20Text-2416-3-10-20201231.pdf}
}



@article{stodden2013setting,
  title={Setting the default to reproducible},
  author={Stodden, Victoria and Borwein, Jonathan and Bailey, David H},
  journal={computational science research. SIAM News},
  volume={46},
  number={5},
  pages={4--6},
  year={2013},
  file={Biblio/DomainSpecific/Reproducibility/icerm_report.pdf}
}



@article{stodden2010scientific,
  title={The scientific method in practice: Reproducibility in the computational sciences},
  author={Stodden, Victoria},
  year={2010},
  publisher={MIT Sloan research paper},
  file={Biblio/DomainSpecific/Reproducibility/SSRN-id1550193.pdf}
}




@article{Nicholse2100769118,
	author = {Nichols, James D. and Oli, Madan K. and Kendall, William. L. and Boomer, G. Scott},
	title = {Opinion: A better approach for dealing with reproducibility and replicability in science},
	volume = {118},
	number = {7},
	elocation-id = {e2100769118},
	year = {2021},
	doi = {10.1073/pnas.2100769118},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/7/e2100769118},
	eprint = {https://www.pnas.org/content/118/7/e2100769118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Biblio/DomainSpecific/Reproducibility/e2100769118.full.pdf}
}



@article{perkel2020challenge,
title={Challenge to scientists: does your ten-year-old code still run?},
author={Perkel, J.M.},
journal={Nature},
volume={584},
pages={656-658},
year={2020},
doi={10.1038/d41586-020-02462-7},
file={Biblio/DomainSpecific/Reproducibility/d41586-020-02462-7.pdf},
url={https://www.nature.com/articles/d41586-020-02462-7}
}



@article{Yang10762,
	author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
	title = {Estimating the deep replicability of scientific findings using human and artificial intelligence},
	volume = {117},
	number = {20},
	pages = {10762--10768},
	year = {2020},
	doi = {10.1073/pnas.1909046117},
	publisher = {National Academy of Sciences},
	abstract = {After years of urgent concern about the failure of scientific papers to replicate, an accurate, scalable method for identifying findings at risk has yet to arrive. We present a method that combines machine intelligence and human acumen for estimating a study{\textquoteright}s likelihood of replication. Our model{\textemdash}trained and tested on hundreds of manually replicated studies and out-of-sample datasets {\textemdash}is comparable to the best current methods, yet reduces the strain on researchers{\textquoteright} resources. In practice, our model can complement prediction market and survey replication methods, prioritize studies for expensive manual replication tests, and furnish independent feedback to researchers prior to submitting a study for review.Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literature as quickly as replicating papers. This dynamic weakens the literature, raises research costs, and demonstrates the need for new approaches for estimating a study{\textquoteright}s replicability. Here, we trained an artificial intelligence model to estimate a paper{\textquoteright}s replicability using ground truth data on studies that had passed or failed manual replication tests, and then tested the model{\textquoteright}s generalizability on an extensive set of out-of-sample studies. The model predicts replicability better than the base rate of reviewers and comparably as well as prediction markets, the best present-day method for predicting replicability. In out-of-sample tests on manually replicated papers from diverse disciplines and methods, the model had strong accuracy levels of 0.65 to 0.78. Exploring the reasons behind the model{\textquoteright}s predictions, we found no evidence for bias based on topics, journals, disciplines, base rates of failure, persuasion words, or novelty words like {\textquotedblleft}remarkable{\textquotedblright} or {\textquotedblleft}unexpected.{\textquotedblright} We did find that the model{\textquoteright}s accuracy is higher when trained on a paper{\textquoteright}s text rather than its reported statistics and that n-grams, higher order word combinations that humans have difficulty processing, correlate with replication. We discuss how combining human and machine intelligence can raise confidence in research, provide research self-assessment techniques, and create methods that are scalable and efficient enough to review the ever-growing numbers of publications{\textemdash}a task that entails extensive human resources to accomplish with prediction markets and manual replication alone.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/20/10762},
	eprint = {https://www.pnas.org/content/117/20/10762.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Biblio/DomainSpecific/Reproducibility/10.1073@pnas.1909046117.pdf}
}


@article{roesch2020new,
title={New journal for reproduction and replication results},
author={Roesch, E.B. and Rougier, N.},
journal={Nature},
volume={581},
pages={30},
year={2020},
doi={10.1038/d41586-020-01328-2},
url={https://www.nature.com/articles/d41586-020-01328-2},
file={Biblio/DomainSpecific/Reproducibility/d41586-020-01328-2.pdf}
}


@article{10.1371/journal.pbio.3000691,
    author = {Nosek, Brian A. AND Errington, Timothy M.},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {What is replication?},
    year = {2020},
    month = {03},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pbio.3000691},
    pages = {1-8},
    abstract = {What is replication? This Perspective article proposes that the answer shifts the conception of replication from a boring, uncreative, housekeeping activity to an exciting, generative, vital contributor to research progress.},
    number = {3},
    doi = {10.1371/journal.pbio.3000691},
    file={Biblio/DomainSpecific/Reproducibility/journal.pbio.3000691.pdf}
}



@article{Wilson5559,
	author = {Wilson, Brent M. and Harris, Christine R. and Wixted, John T.},
	title = {Science is not a signal detection problem},
	volume = {117},
	number = {11},
	pages = {5559--5567},
	year = {2020},
	doi = {10.1073/pnas.1914237117},
	publisher = {National Academy of Sciences},
	abstract = {The perceived replication crisis and the reforms designed to address it are grounded in the notion that science is a binary signal detection problem. However, contrary to null hypothesis significance testing (NHST) logic, the magnitude of the underlying effect size for a given experiment is best conceptualized as a random draw from a continuous distribution, not as a random draw from a dichotomous distribution (null vs. alternative). Moreover, because continuously distributed effects selected using a P \&lt; 0.05 filter must be inflated, the fact that they are smaller when replicated (reflecting regression to the mean) is no reason to sound the alarm. Considered from this perspective, recent replication efforts suggest that most published P \&lt; 0.05 scientific findings are {\textquotedblleft}true{\textquotedblright} (i.e., in the correct direction), with observed effect sizes that are inflated to varying degrees. We propose that original science is a screening process, one that adopts NHST logic as a useful fiction for selecting true effects that are potentially large enough to be of interest to other scientists. Unlike original science, replication science seeks to precisely measure the underlying effect size associated with an experimental protocol via large-N direct replication, without regard for statistical significance. Registered reports are well suited to (often resource-intensive) direct replications, which should focus on influential findings and be published regardless of outcome. Conceptual replications play an important but separate role in validating theories. However, because they are part of NHST-based original science, conceptual replications cannot serve as the field{\textquoteright}s self-correction mechanism. Only direct replications can do that.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/11/5559},
	eprint = {https://www.pnas.org/content/117/11/5559.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Biblio/DomainSpecific/Reproducibility/10.1073@pnas.1914237117.pdf}
}



@book{kitzes2017practice,
  title={The practice of reproducible research: case studies and lessons from the data-intensive sciences},
  author={Kitzes, Justin and Turek, Daniel and Deniz, Fatma},
  year={2017},
  publisher={Univ of California Press},
  file={Biblio/DomainSpecific/Reproducibility/the-practice-of-reproducible-research.pdf}
}


@article{nature2020irreproducibility,
title={Irreproducibility is not a sign of failure, but an inspiration for fresh ideas},
author={Nature Editorial}
journal={Nature},
volume={578},
pages={191-192},
year={2020},
file={Biblio/DomainSpecific/Reproducibility/d41586-020-00380-2.pdf},
url={https://www.nature.com/articles/d41586-020-00380-2},
doi={10.1038/d41586-020-00380-2}
}


@article{chawla2020software,
title={Software searches out reproducibility issues in scientific papers},
author={Chawla, D.S.},
journal={Nature},
year={2020},
doi={10.1038/d41586-020-00104-6},
url={https://www.nature.com/articles/d41586-020-00104-6}
}

@article{Bryan25535,
	author = {Bryan, Christopher J. and Yeager, David S. and O{\textquoteright}Brien, Joseph M.},
	title = {Replicator degrees of freedom allow publication of misleading failures to replicate},
	volume = {116},
	number = {51},
	pages = {25535--25545},
	year = {2019},
	doi = {10.1073/pnas.1910951116},
	publisher = {National Academy of Sciences},
	abstract = {We show that commonly exercised flexibility at the experimental design and data analysis stages of replication testing makes it easy to publish false-negative replication results while maintaining the impression of methodological rigor. These findings have important implications for how the many ostensible nonreplications already in the literature should be interpreted and for how future replication tests should be conducted.In recent years, the field of psychology has begun to conduct replication tests on a large scale. Here, we show that {\textquotedblleft}replicator degrees of freedom{\textquotedblright} make it far too easy to obtain and publish false-negative replication results, even while appearing to adhere to strict methodological standards. Specifically, using data from an ongoing debate, we show that commonly exercised flexibility at the experimental design and data analysis stages of replication testing can make it appear that a finding was not replicated when, in fact, it was. The debate that we focus on is representative, on key dimensions, of a large number of other replication tests in psychology that have been published in recent years, suggesting that the lessons of this analysis may be far reaching. The problems with current practice in replication science that we uncover here are particularly worrisome because they are not adequately addressed by the field{\textquoteright}s standard remedies, including preregistration. Implications for how the field could develop more effective methodological standards for replication are discussed.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/116/51/25535},
	eprint = {https://www.pnas.org/content/116/51/25535.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	file={Biblio/DomainSpecific/Reproducibility/25535.full.pdf}
}


@article{hinsen2019challenge,
author={Hinsen, K. and Rougier, N.},
title={Challenge to test reproducibility of old computer code},
journal={Nature},
volume={574},
pages={634}
year={2019},
doi={10.1038/d41586-019-03296-8},file={Biblio/DomainSpecific/Reproducibility/
file={Biblio/DomainSpecific/Reproducibility/d41586-019-03296-8.pdf},
url={https://www.nature.com/articles/d41586-019-03296-8}
}


@article{patil2016statistical,
  title={A statistical definition for reproducibility and replicability},
  author={Patil, Prasad and Peng, Roger D and Leek, Jeffrey},
  journal={BioRxiv},
  pages={066803},
  year={2016},
  publisher={Cold Spring Harbor Laboratory},
  file={Biblio/DomainSpecific/Reproducibility/066803.full.pdf}
}




@inproceedings{dragicevic:hal-01976951,
  TITLE = {{Increasing the Transparency of Research Papers with Explorable Multiverse Analyses}},
  AUTHOR = {Dragicevic, Pierre and Jansen, Yvonne and Sarma, Abhraneel and Kay, Matthew and Chevalier, Fanny},
  URL = {https://hal.inria.fr/hal-01976951},
  BOOKTITLE = {{CHI 2019 - The ACM CHI Conference on Human Factors in Computing Systems}},
  ADDRESS = {Glasgow, United Kingdom},
  YEAR = {2019},
  MONTH = May,
  DOI = {10.1145/3290605.3300295},
  KEYWORDS = {multiverse analysis ; explorable explanation ; statistics ; transparent reporting ; interactive documents},
  PDF = {https://hal.inria.fr/hal-01976951/file/Interactive_Paper_CHI2019_camera_ready.pdf},
  HAL_ID = {hal-01976951},
  HAL_VERSION = {v1},
  file={Biblio/DomainSpecific/Reproducibility/Interactive_Paper_CHI2019_camera_ready.pdf}
}



@article{perkel2019pioneering,
  title={Pioneering ‘live-code’ article allows scientists to play with each other’s results},
  author={Perkel, J.M.},
  journal={Nature},
  year={2019},
  volume={567},
  pages={17--18},
  file={Biblio/DomainSpecific/Reproducibility/d41586-019-00724-7.pdf},
  url={https://www.nature.com/articles/d41586-019-00724-7}
}


@article{govoni2019qresp,
  title={Qresp, a tool for curating, discovering and exploring reproducible scientific papers},
  author={Govoni, Marco and Munakami, Milson and Tanikanti, Aditya and Skone, Jonathan H and Runesha, Hakizumwami B and Giberti, Federico and de Pablo, Juan and Galli, Giulia},
  journal={Scientific Data},
  volume={6},
  pages={190002},
  year={2019},
  publisher={Nature Publishing Group},
  file={Biblio/DomainSpecific/Reproducibility/sdata20192.pdf}
}


@article{juergens2018evaluation,
	Author = {Juergens, Hannes and Niemeijer, Matthijs and Jennings-Antipov, Laura D. and Mans, Robert and Morel, Jack and van Maris, Antonius J. A. and Pronk, Jack T. and Gardner, Timothy S.},
	Journal = {Scientific Data},
	Month = {10},
	Pages = {180195 EP  -},
	Title = {Evaluation of a novel cloud-based software platform for structured experiment design and linked data analytics},
	Volume = {5},
	Year = {2018}
	file={Biblio/DomainSpecific/Reproducibility/sdata2018195.pdf}
}





@article{perkel2018toolkit,
  author={J. M. Perkel},
  title={A toolkit for data transparency takes shape},
  journal={Nature},
  pages={513--515},
  volume={560},
  doi={10.1038/d41586-018-05990-5},
  year={2018},
  file={Biblio/DomainSpecific/Reproducibility/d41586-018-05990-5.pdf}
}



@ARTICLE{2018arXiv180500400B,
   author = {{Brinckman}, A. and {Chard}, K. and {Gaffney}, N. and {Hategan}, M. and 
	{Jones}, M.~B. and {Kowalik}, K. and {Kulasekaran}, S. and {Lud{\"a}scher}, B. and 
	{Mecum}, B.~D. and {Nabrzyski}, J. and {Stodden}, V. and {Taylor}, I.~J. and 
	{Turk}, M.~J. and {Turner}, K.},
    title = "{Computing Environments for Reproducibility: Capturing the ``Whole Tale''}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1805.00400},
 primaryClass = "cs.CY",
 keywords = {Computer Science - Computers and Society},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180500400B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  file={Biblio/DomainSpecific/Reproducibility/1805.00400.pdf}
}





@article{shiffrin2018scientific,
	  title={Scientific progress despite irreproducibility: A seeming paradox},
	    author={Shiffrin, Richard M and B{\"o}rner, Katy and Stigler, Stephen M},
	      journal={Proceedings of the National Academy of Sciences},
	        volume={115},
		  number={11},
		    pages={2632--2639},
		      year={2018},
		        publisher={National Acad Sciences},
			file={Biblio/DomainSpecific/Reproducibility/2632.full.pdf}
}




@article{fanelli2018opinion,
	  title={Opinion: Is science really facing a reproducibility crisis, and do we need it to?},
	    author={Fanelli, Daniele},
	      journal={Proceedings of the National Academy of Sciences},
	        volume={115},
		  number={11},
		    pages={2628--2631},
		      year={2018},
		        publisher={National Acad Sciences},
			file={Biblio/DomainSpecific/Reproducibility/2628.full.pdf}
}



@article{boutron2018misrepresentation,
	  title={Misrepresentation and distortion of research in biomedical literature},
	    author={Boutron, Isabelle and Ravaud, Philippe},
	      journal={Proceedings of the National Academy of Sciences},
	        volume={115},
		  number={11},
		    pages={2613--2619},
		      year={2018},
		        publisher={National Acad Sciences},
			file={Biblio/DomainSpecific/Reproducibility/2613.full.pdf}
}




@article{stodden2018empirical,
	  title={An empirical analysis of journal policy effectiveness for computational reproducibility},
	    author={Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
	      journal={Proceedings of the National Academy of Sciences},
	        volume={115},
		  number={11},
		    pages={2584--2589},
		      year={2018},
		        publisher={National Acad Sciences},
			file={Biblio/DomainSpecific/Reproducibility/2584.full.pdf}
}



